{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: CNNs and Transfer Learning\n",
    "Name(s): Scott Charles\n",
    "\n",
    "Note: the report with the following headings can be completed in this notebook, or in a separate document.\n",
    "\n",
    "## 1. Dataset Description\n",
    "A description of the dataset and the class you chose to predict, including:\n",
    "1. Biases and limitations of the dataset\n",
    "2. Class imbalance\n",
    "3. A summary of your impressions of the dataset\n",
    "\n",
    "## 2. Basic Model\n",
    "A description of the model you built from scratch, including:\n",
    "1. The architecture of the model\n",
    "2. The loss function and optimizer you used\n",
    "3. The metrics you used to evaluate the model\n",
    "4. A discussion of how you approached building, training, and refining the model\n",
    "\n",
    "## 3. Transfer Learning Model\n",
    "A description of the transfer learning model, including:\n",
    "1. A reference to the pre-trained model you used\n",
    "2. Why you chose that model\n",
    "3. A discussion of how you approached adding and training the new layers\n",
    "\n",
    "## 4. Discussion/Conclusion\n",
    "A discussion/conclusion section describing:\n",
    "1. How the two models compared in terms of performance and ease of creation\n",
    "2. Challenges, advantages, and limitations of each approach\n",
    "3. Which you would choose if you were deploying this model in a production environment\n",
    "4. Any other thoughts or observations you have about the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the things\n",
    "# Note that you will need to pip install tensorflow-datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The handy tfds.load function isn't working due to a [known issue](https://github.com/tensorflow/datasets/issues/1482), so follow these steps to get the data (~1.5GB):\n",
    "\n",
    "### Option 1: On your local machine\n",
    "\n",
    "1. Download these 5 urls:\n",
    "    | URL | Size | Checksum | filename | \n",
    "    | --- | ---- | ------- | ----------- |\n",
    "    | https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pY0NSMzRuSXJEVkk | 2836386 | fc955bcb3ef8fbdf7d5640d9a8693a8431b5f2ee291a5c1449a1549e7e073fe7 | list_eval_partition.txt |\n",
    "    | https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM | 1443490838 | 46fb89443c578308acf364d7d379fe1b9efb793042c0af734b6112e4fd3a8c74 | img_align_celeba.zip |\n",
    "    | https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pblRyaVFSWGxPY0U | 26721026 | f0e5da289d5ccf75ffe8811132694922b60f2af59256ed362afa03fefba324d0 | list_attr_celeba.txt |\n",
    "    | https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pd0FJY3Blby1HUTQ | 12156055 | 6c02a87569907f6db2ba99019085697596730e8129f67a3d61659f198c48d43b | list_landmarks_align_celeba.txt |\n",
    "    | https://drive.google.com/uc?export=download&id=1_ee_0u7vcNLOfNLegJRHmolfH5ICW-XS | 3424458 | c6143857c3e2630ac2da9f782e9c1232e5e59be993a9d44e8a7916c78a6158c0 | identity_CelebA.txt |\n",
    "2. Move the files to `~/tensorflow_datasets/downloads/manual/`\n",
    "\n",
    "At this stage your directory structure should look like this:\n",
    "```bash\n",
    "~\n",
    "└── tensorflow_datasets\n",
    "    └── downloads\n",
    "        └── manual\n",
    "            ├── identity_CelebA.txt\n",
    "            ├── img_align_celeba.zip\n",
    "            ├── list_attr_celeba.txt\n",
    "            ├── list_eval_partition.txt\n",
    "            └── list_landmarks_align_celeba.txt\n",
    "```\n",
    "If there are already things in the `manual` directory, that's fine. Just add the files to it.\n",
    "\n",
    "### Option 2: On Google Colab\n",
    "1. Download the same 5 files to your local machine\n",
    "2. Upload the files to your Google Drive in a folder named `tensorflow_datasets/downloads/manual/`. The `tensorflow_datasets` folder should be in the root of your Google Drive (unless you want to change the paths in the code below).\n",
    "\n",
    "Run the following cell to mount your Google Drive and configure the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    gdrive_root = '/content/gdrive'\n",
    "    drive.mount(gdrive_root)\n",
    "    data_dir = gdrive_root + '/My Drive/tensorflow_datasets/'\n",
    "except:\n",
    "    COLAB = False\n",
    "    data_dir = '~/tensorflow_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can load the data, but this will still take a while the first time\n",
    "ds_train = tfds.load('celeb_a', split='train[:80%]', shuffle_files=True, data_dir=data_dir)\n",
    "ds_val = tfds.load('celeb_a', split='train[80%:]', shuffle_files=True, data_dir=data_dir)\n",
    "ds_test = tfds.load('celeb_a', split='test', shuffle_files=True, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "Feel free to explore the data in other ways as well. These cells are provided to give some idea of how to interact with the Tensorflow Dataset format.\n",
    "\n",
    "The examples shown here illustrate the \"Eyeglasses\" attribute - pick a **different one** for your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a different attribute for your project!\n",
    "ATTRIBUTE = 'Eyeglasses'\n",
    "\n",
    "# look at some examples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for example, ax in zip(ds_train.take(5), axes):\n",
    "    image, label = example[\"image\"], example[\"attributes\"][ATTRIBUTE]\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f'{ATTRIBUTE}: {label}')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out class balance of a random sample\n",
    "label_count = 0\n",
    "SHUF_BUF = 1024\n",
    "random_sample = 1000\n",
    "\n",
    "# ds.shuffle loads N records (1024 in this case) and then takes the first 1000\n",
    "for record in ds_train.shuffle(SHUF_BUF).take(random_sample):\n",
    "    label_count += int(record[\"attributes\"][ATTRIBUTE])\n",
    "\n",
    "print(f'{label_count} of {random_sample} have {ATTRIBUTE} = True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "I recommend using the following functions to preprocess and sample the data. This is a large and unbalanced dataset, so the `get_balanced_data` function creates a subsample with an equal number of positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep things manageable I recommend downsampling to 128x128\n",
    "# This will distort the images, but that doesn't matter very much\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "def preprocess_input_dict(feat_dict):\n",
    "    \"\"\"\n",
    "    Separates the image and label from the feature dictionary.\n",
    "    \"\"\"\n",
    "    image = feat_dict['image']\n",
    "    label = feat_dict['attributes'][ATTRIBUTE]\n",
    "\n",
    "    # Resize and normalize image.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image /= 255.0\n",
    "\n",
    "    return (image, label)\n",
    "\n",
    "def get_balanced_data(ds, batch_size):\n",
    "    \"\"\"\n",
    "    Returns a balanced dataset with the specified split and batch size.\n",
    "    Maps each sample to the preprocessing function.\n",
    "    \"\"\"\n",
    "    pos_ds = ds.filter(lambda d: d[\"attributes\"][ATTRIBUTE] == True)\n",
    "    neg_ds = ds.filter(lambda d: d[\"attributes\"][ATTRIBUTE] == False)\n",
    "    balanced = tf.data.Dataset.sample_from_datasets(\n",
    "        [pos_ds, neg_ds], \n",
    "        weights=[0.5, 0.5], \n",
    "        stop_on_empty_dataset=True)\n",
    "    balanced = balanced.shuffle(SHUF_BUF).batch(batch_size)\n",
    "    return balanced.map(preprocess_input_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model creation\n",
    "Since we haven't talked about working with the weird [Tensorflow.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) format, I've included a sample model creation process. This is a simple model that uses the `get_balanced_data` function to create a balanced dataset, and then trains a model on it. It's not a good model, and in fact it behaves much better than I expected, but hopefully it provides a starting point for you to build your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of calling the get_balanced_data function\n",
    "# Note that the resulting objects are not pandas dataframes or even numpy arrays,\n",
    "# but tensorflow datasets that have indeterminate size\n",
    "BATCH_SIZE = 32\n",
    "train = get_balanced_data(ds_train, BATCH_SIZE)\n",
    "val = get_balanced_data(ds_val, BATCH_SIZE)\n",
    "test = get_balanced_data(ds_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a whole bunch of metrics you could look at - which ones make the most sense?\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    # add any other metrics here\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore different optimizers, number of epocs, etc\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=metrics)\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=5, \n",
    "    validation_data=val,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the history\n",
    "hist = pd.DataFrame(history.history)\n",
    "plt.plot(hist['accuracy'], label='train')\n",
    "plt.plot(hist['val_accuracy'], label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Finally, here's some functions you can use to help evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion(model):\n",
    "    y_pred = [] \n",
    "    y_true = [] \n",
    "\n",
    "    # iterate over the dataset\n",
    "    for image_batch, label_batch in test:\n",
    "        y_true += list(label_batch)\n",
    "        preds = model.predict(image_batch)\n",
    "        y_pred += list(preds > 0.5)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f'Not {ATTRIBUTE}', ATTRIBUTE])\n",
    "    disp.plot()\n",
    "\n",
    "plot_confusion(model)\n",
    "\n",
    "# you can also call model.evaluate to calculate all the metrics at once\n",
    "test_metrics = model.evaluate(test)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Model\n",
    "Building and training a transfer learning model is much like doing it from scratch, but before calling model.compile you need to freeze the layers of the pre-trained model. This is done by setting the `trainable` attribute of the layers to `False`. You can then add new layers to the model and train them as usual.\n",
    "\n",
    "Basic process:\n",
    "1. Choose a pre-trained model (I recommend sticking to the [Keras Applications models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) to keep it simple)\n",
    "2. Load the model and set `trainable=False` for all layers\n",
    "3. Add new layers to the model to finalize the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of how to use a pre-trained model\n",
    "base_model = tf.keras.applications.VGG16(\n",
    "    include_top=False, \n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    ")\n",
    "\n",
    "# freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add on the classification \"head\"\n",
    "flatten_layer = tf.keras.layers.Flatten()(base_model.output)\n",
    "dense_layer = tf.keras.layers.Dense(64, activation='relu')(flatten_layer)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
    "transfer_model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# compile the same way\n",
    "transfer_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train as usual\n",
    "transfer_history = transfer_model.fit(\n",
    "    train,\n",
    "    epochs=5, \n",
    "    validation_data=val,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
